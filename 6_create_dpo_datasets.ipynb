{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/incomple_/anaconda3/envs/llamafactory/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] Loaded 2246 samples from results/initial_eval_results_Llama-3.1-8B-Instruct.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building baseline pairs: 100%|██████████| 2246/2246 [00:00<00:00, 516838.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# 6_create_dpo_datasets.ipynb\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "# DPO Dataset Construction (Preference-Pair Style) with qn_id and Train/Test Split\n",
    "\n",
    "This notebook:\n",
    "\n",
    "1. Loads an initial_eval JSON (like `results/initial_eval_results_Llama-3.1-8B-Instruct.json`) \n",
    "   and a stance-change directory (like `results/stance_change_Llama-3.1-8B-Instruct_intended-response`)\n",
    "   to build a DPO dataset of preference pairs.\n",
    "\n",
    "2. Incorporates `qn_id` from the JSON data (which presumably has it after \n",
    "   you assigned question IDs in your earlier step).\n",
    "\n",
    "3. Reads `data/qn_id_split_llama31_8b.json` which has \n",
    "   `{\"train_ids\": [...], \"test_ids\": [...], \"stats\": {...}}`\n",
    "   to see if each row belongs in the train or test set.\n",
    "\n",
    "4. Prints statistics about how many items are in each category (e.g. baseline vs. resist vs. relent) \n",
    "   and each subset (train/test).\n",
    "\n",
    "5. Saves four different versions of the final DPO data:\n",
    "   - baseline only\n",
    "   - (baseline + resist_NEG)\n",
    "   - (baseline + resist_NEG + relent_POS) [= \"holistic\"]\n",
    "   - also if you want only \"relent_POS\" alone or all four sets, adapt as needed.\n",
    "\n",
    "6. For each version, saves both:\n",
    "   - a CSV format\n",
    "   - a JSON in DPO format:\n",
    "     ```\n",
    "     [\n",
    "       {\n",
    "         \"conversations\": [\n",
    "           {\"from\": \"human\", \"value\": \"...\"},\n",
    "           {\"from\": \"gpt\", \"value\": \"...\"},\n",
    "           ...\n",
    "         ],\n",
    "         \"chosen\": {\"from\": \"gpt\", \"value\": \"...\"},\n",
    "         \"rejected\": {\"from\": \"gpt\", \"value\": \"...\"}\n",
    "       }\n",
    "     ]\n",
    "     ```\n",
    "   - plus a **KTO** version that duplicates each row as two samples \n",
    "     (one with `kto_tag=true` for the chosen, one with `kto_tag=false` for the rejected).\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import gzip\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# PATHS\n",
    "# ------------------------------------------------------------------------------\n",
    "INITIAL_EVAL_JSON = \"results/initial_eval_results_Llama-3.1-8B-Instruct.json\"\n",
    "STANCE_CHANGE_DIR = \"results/stance_change_Llama-3.1-8B-Instruct_intended-response\"\n",
    "QNSPLIT_JSON = \"data/qn_id_split_llama31_8b.json\"  # has train_ids/test_ids\n",
    "\n",
    "OUT_DIR = \"dpo_datasets\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# We'll gather final preference pairs here\n",
    "dpo_rows = []\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1) Load initial_eval, build baseline pairs\n",
    "# ------------------------------------------------------------------------------\n",
    "with open(INITIAL_EVAL_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    initial_data = json.load(f)\n",
    "\n",
    "print(f\"[Baseline] Loaded {len(initial_data)} samples from {INITIAL_EVAL_JSON}...\")\n",
    "\n",
    "for sample in tqdm(initial_data, desc=\"Building baseline pairs\"):\n",
    "    # We assume each sample has \"qn_id\"\n",
    "    qn_id = sample.get(\"qn_id\", \"NOT_FOUND\")\n",
    "\n",
    "    conversation = [\n",
    "        {\"role\": \"user\", \"content\": sample.get(\"initial_prompt\", \"\")}\n",
    "    ]\n",
    "\n",
    "    correct_idx = sample[\"answer_idx\"]\n",
    "    correct_letter = string.ascii_uppercase[correct_idx]\n",
    "    source = sample.get(\"source\", \"N/A\")\n",
    "    question = sample.get(\"question\", \"N/A\")\n",
    "    category = sample.get(\"category\", \"N/A\")\n",
    "\n",
    "    if sample.get(\"initial_correct\", False):\n",
    "        t_idx = sample[\"target_idx\"]\n",
    "        wrong_letter = string.ascii_uppercase[t_idx]\n",
    "    else:\n",
    "        wrong_letter = sample.get(\"initial_model_letter\")\n",
    "\n",
    "    row_baseline = {\n",
    "        \"qn_id\": qn_id,\n",
    "        \"conversations\": conversation,  # single user turn\n",
    "        \"chosen\": correct_letter,\n",
    "        \"rejected\": wrong_letter,\n",
    "        \"appeal_seting\": \"N/A\",\n",
    "        \"appeal_technique\": \"N/A\",\n",
    "        \"num_turns\": 0,\n",
    "        \"sample_subset\": \"baseline\",\n",
    "        \"source\": source,\n",
    "        \"question\": question,\n",
    "        \"category\": category,\n",
    "        \"correct_letter\": correct_letter,\n",
    "        \"target_letter\": string.ascii_uppercase[sample[\"target_idx\"]],\n",
    "        \"initial_model_letter\": sample.get(\"initial_model_letter\")\n",
    "    }\n",
    "    dpo_rows.append(row_baseline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Resist/Relent] Found 7 JSON files in results/stance_change_Llama-3.1-8B-Instruct_intended-response...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2) Build Resist (NEG) / Relent (POS) from stance_change dir\n",
    "# ------------------------------------------------------------------------------\n",
    "stance_files = glob(os.path.join(STANCE_CHANGE_DIR, \"*.json\"))\n",
    "print(f\"[Resist/Relent] Found {len(stance_files)} JSON files in {STANCE_CHANGE_DIR}...\")\n",
    "\n",
    "def build_conversation_prefix(sample):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": sample.get(\"initial_prompt\", \"\")},\n",
    "        {\"role\": \"assistant\", \"content\": sample.get(\"initial_model_answer\", \"\")}\n",
    "    ]\n",
    "\n",
    "for filepath in stance_files:\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        main_data = json.load(f)\n",
    "    samples = main_data.get(\"samples\", [])\n",
    "\n",
    "    # Extract technique from filename\n",
    "    # E.g. from initial_eval_results_Llama-3.1-8B-Instruct_Authority Endorsement_refuted_affirmed.json, get \"Authority Endorsement\"\n",
    "    # E.g. from initial_eval_results_Mistral-7B-Instruct-v0.3_Authority Endorsement_refuted_affirmed.json, get \"Authority Endorsement\"\n",
    "    stem = os.path.basename(filepath).replace(\"_refuted_affirmed.json\", \"\")\n",
    "    # Technique tag is the part after the last underscore\n",
    "    technique_tag = stem.split(\"_\")[-1]\n",
    "\n",
    "    \n",
    "    for sample in samples:\n",
    "        qn_id = sample.get(\"qn_id\", \"NOT_FOUND\")\n",
    "        attempts = sample.get(\"persuasion_attempts\", [])\n",
    "        n = min(len(attempts), 3)\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        source = sample.get(\"source\", \"N/A\")\n",
    "        question = sample.get(\"question\", \"N/A\")\n",
    "        category = sample.get(\"category\", \"N/A\")\n",
    "\n",
    "        correct_idx = sample[\"answer_idx\"]\n",
    "        correct_letter = string.ascii_uppercase[correct_idx]\n",
    "        target_letter = string.ascii_uppercase[sample[\"target_idx\"]]\n",
    "        initial_model_letter = sample.get(\"initial_model_letter\")\n",
    "        init_correct = sample.get(\"initial_correct\", False)\n",
    "\n",
    "        if init_correct:\n",
    "            # Resist scenario\n",
    "            appeal_setting = \"NEG\"\n",
    "            subset_tag = \"resist_NEG\"\n",
    "            neg_letter = target_letter\n",
    "        else:\n",
    "            # Relent scenario\n",
    "            appeal_setting = \"POS\"\n",
    "            subset_tag = \"relent_POS\"\n",
    "            neg_letter = initial_model_letter  # old \"incorrect\" letter for final stance check\n",
    "\n",
    "        def build_conversation_up_to(i):\n",
    "            conv = build_conversation_prefix(sample)\n",
    "            for j in range(i):\n",
    "                att = attempts[j]\n",
    "                user_j = att[\"persuasion_user_text\"]\n",
    "                intended_j = att.get(\"intended_response\", \"[No intended response]\")\n",
    "                conv.append({\"role\": \"user\", \"content\": user_j})\n",
    "                conv.append({\"role\": \"assistant\", \"content\": intended_j})\n",
    "            return conv\n",
    "\n",
    "        for i_at in range(n):\n",
    "            att = attempts[i_at]\n",
    "            user_i = att[\"persuasion_user_text\"]\n",
    "            original_i = att[\"persuasion_assistant_text\"]\n",
    "            intended_i = att.get(\"intended_response\", \"[No intended response]\")\n",
    "            stance_user = att[\"stance_check_user_text\"]\n",
    "            stance_asst = att[\"stance_check_assistant_text\"]\n",
    "\n",
    "            # Pair 1: compare chosen= intended_i vs. rejected= original_i\n",
    "            conv_ref = build_conversation_up_to(i_at)\n",
    "            conv_ref.append({\"role\": \"user\", \"content\": user_i})\n",
    "            row_ref = {\n",
    "                \"qn_id\": qn_id,\n",
    "                \"conversations\": conv_ref,\n",
    "                \"chosen\": intended_i,\n",
    "                \"rejected\": original_i,\n",
    "                \"appeal_technique\": technique_tag,\n",
    "                \"appeal_setting\": appeal_setting,\n",
    "                \"num_turns\": i_at+1,\n",
    "                \"sample_subset\": subset_tag,\n",
    "                \"source\": source,\n",
    "                \"category\": category,\n",
    "                \"correct_letter\": correct_letter,\n",
    "                \"target_letter\": target_letter,\n",
    "                \"initial_model_letter\": initial_model_letter\n",
    "            }\n",
    "            dpo_rows.append(row_ref)\n",
    "\n",
    "            # Pair 2: final stance check => chosen= correct_letter, rejected= neg_letter\n",
    "            conv_stance = build_conversation_up_to(i_at)\n",
    "            conv_stance.append({\"role\": \"user\", \"content\": user_i})\n",
    "            conv_stance.append({\"role\": \"assistant\", \"content\": intended_i})\n",
    "            conv_stance.append({\"role\": \"user\", \"content\": stance_user})\n",
    "\n",
    "            row_stance = {\n",
    "                \"qn_id\": qn_id,\n",
    "                \"conversations\": conv_stance,\n",
    "                \"chosen\": correct_letter,\n",
    "                \"rejected\": neg_letter,\n",
    "                \"appeal_technique\": technique_tag,\n",
    "                \"appeal_setting\": appeal_setting,\n",
    "                \"num_turns\": i_at+1,\n",
    "                \"sample_subset\": subset_tag,\n",
    "                \"source\": source,\n",
    "                \"category\": category,\n",
    "                \"correct_letter\": correct_letter,\n",
    "                \"target_letter\": target_letter,\n",
    "                \"initial_model_letter\": initial_model_letter\n",
    "            }\n",
    "            dpo_rows.append(row_stance)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total preference pairs: 96578\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qn_id</th>\n",
       "      <th>conversations</th>\n",
       "      <th>chosen</th>\n",
       "      <th>rejected</th>\n",
       "      <th>appeal_seting</th>\n",
       "      <th>appeal_technique</th>\n",
       "      <th>num_turns</th>\n",
       "      <th>sample_subset</th>\n",
       "      <th>source</th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "      <th>correct_letter</th>\n",
       "      <th>target_letter</th>\n",
       "      <th>initial_model_letter</th>\n",
       "      <th>appeal_setting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mmlu-pro_psychology_0c9d440b</td>\n",
       "      <td>[{'role': 'user', 'content': 'Answer the follo...</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>baseline</td>\n",
       "      <td>mmlu-pro</td>\n",
       "      <td>Which of the following vocational interest inv...</td>\n",
       "      <td>psychology</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mmlu-pro_psychology_6f27e652</td>\n",
       "      <td>[{'role': 'user', 'content': 'Answer the follo...</td>\n",
       "      <td>J</td>\n",
       "      <td>B</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>baseline</td>\n",
       "      <td>mmlu-pro</td>\n",
       "      <td>What is meant by the termconsciousness ?</td>\n",
       "      <td>psychology</td>\n",
       "      <td>J</td>\n",
       "      <td>B</td>\n",
       "      <td>J</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mmlu-pro_psychology_6d600e82</td>\n",
       "      <td>[{'role': 'user', 'content': 'Answer the follo...</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>baseline</td>\n",
       "      <td>mmlu-pro</td>\n",
       "      <td>A major problem in thinking is maintaining ale...</td>\n",
       "      <td>psychology</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mmlu-pro_psychology_a909eee4</td>\n",
       "      <td>[{'role': 'user', 'content': 'Answer the follo...</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>baseline</td>\n",
       "      <td>mmlu-pro</td>\n",
       "      <td>Which of the following is most likely to produ...</td>\n",
       "      <td>psychology</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mmlu-pro_psychology_f851d902</td>\n",
       "      <td>[{'role': 'user', 'content': 'Answer the follo...</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>baseline</td>\n",
       "      <td>mmlu-pro</td>\n",
       "      <td>What are the four tenets of analytical psychot...</td>\n",
       "      <td>psychology</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          qn_id  \\\n",
       "0  mmlu-pro_psychology_0c9d440b   \n",
       "1  mmlu-pro_psychology_6f27e652   \n",
       "2  mmlu-pro_psychology_6d600e82   \n",
       "3  mmlu-pro_psychology_a909eee4   \n",
       "4  mmlu-pro_psychology_f851d902   \n",
       "\n",
       "                                       conversations chosen rejected  \\\n",
       "0  [{'role': 'user', 'content': 'Answer the follo...      B        A   \n",
       "1  [{'role': 'user', 'content': 'Answer the follo...      J        B   \n",
       "2  [{'role': 'user', 'content': 'Answer the follo...      F        C   \n",
       "3  [{'role': 'user', 'content': 'Answer the follo...      A        B   \n",
       "4  [{'role': 'user', 'content': 'Answer the follo...      F        C   \n",
       "\n",
       "  appeal_seting appeal_technique  num_turns sample_subset    source  \\\n",
       "0           N/A              N/A          0      baseline  mmlu-pro   \n",
       "1           N/A              N/A          0      baseline  mmlu-pro   \n",
       "2           N/A              N/A          0      baseline  mmlu-pro   \n",
       "3           N/A              N/A          0      baseline  mmlu-pro   \n",
       "4           N/A              N/A          0      baseline  mmlu-pro   \n",
       "\n",
       "                                            question    category  \\\n",
       "0  Which of the following vocational interest inv...  psychology   \n",
       "1           What is meant by the termconsciousness ?  psychology   \n",
       "2  A major problem in thinking is maintaining ale...  psychology   \n",
       "3  Which of the following is most likely to produ...  psychology   \n",
       "4  What are the four tenets of analytical psychot...  psychology   \n",
       "\n",
       "  correct_letter target_letter initial_model_letter appeal_setting  \n",
       "0              B             A                    A            NaN  \n",
       "1              J             B                    J            NaN  \n",
       "2              F             C                    F            NaN  \n",
       "3              A             B                    A            NaN  \n",
       "4              F             C                    C            NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 3) Create a DataFrame\n",
    "# ------------------------------------------------------------------------------\n",
    "df = pd.DataFrame(dpo_rows)\n",
    "print(f\"Total preference pairs: {len(df)}\")\n",
    "df.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Subset distribution ###\n",
      "sample_subset  split\n",
      "baseline       test      1377\n",
      "               train      345\n",
      "relent_POS     test     31794\n",
      "               train     7896\n",
      "resist_NEG     test     26040\n",
      "               train     6594\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # ------------------------------------------------------------------------------\n",
    "# # 4) Use data/qn_id_split_llama31_8b.json to see if train or test\n",
    "# # ------------------------------------------------------------------------------\n",
    "# with open(QNSPLIT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "#     split_data = json.load(f)\n",
    "# train_ids = set(split_data[\"train_ids\"])\n",
    "# test_ids  = set(split_data[\"test_ids\"])\n",
    "\n",
    "# def get_split(qn_id):\n",
    "#     if qn_id in train_ids:\n",
    "#         return \"train\"\n",
    "#     elif qn_id in test_ids:\n",
    "#         return \"test\"\n",
    "#     else:\n",
    "#         return \"unspecified\"\n",
    "\n",
    "# df[\"split\"] = df[\"qn_id\"].apply(get_split)\n",
    "\n",
    "\n",
    "# # ------------------------------------------------------------------------------\n",
    "# # Print stats\n",
    "# # ------------------------------------------------------------------------------\n",
    "# # For instance, how many rows are baseline vs. resist_NEG vs. relent_POS, \n",
    "# # splitted by train/test\n",
    "# grouped = df.groupby([\"sample_subset\", \"split\"]).size()\n",
    "# print(\"\\n### Subset distribution ###\")\n",
    "# print(grouped)\n",
    "\n",
    "\n",
    "# # ------------------------------------------------------------------------------\n",
    "# # Save four subsets in both CSV and JSON\n",
    "# #   1) baseline only\n",
    "# #   2) baseline+resist_NEG\n",
    "# #   3) baseline+resist_NEG+relent_POS (holistic)\n",
    "# #   4) possibly relent_POS alone, or skip if not needed\n",
    "# # ------------------------------------------------------------------------------\n",
    "# import os\n",
    "# import json\n",
    "# import gzip\n",
    "\n",
    "# def filter_and_save(name, condition, use_gzip=False):\n",
    "#     # Apply the condition + ensure only train split\n",
    "#     subdf = df[condition & (df[\"split\"] == \"train\")].copy()\n",
    "    \n",
    "#     if subdf.empty:\n",
    "#         print(f\"Skipping {name}, no train samples found.\")\n",
    "#         return\n",
    "\n",
    "#     # CSV file\n",
    "#     csv_ext = \".csv.gz\" if use_gzip else \".csv\"\n",
    "#     csv_path = os.path.join(OUT_DIR, f\"{name}{csv_ext}\")\n",
    "#     if use_gzip:\n",
    "#         with gzip.open(csv_path, \"wt\", encoding=\"utf-8\") as f:\n",
    "#             subdf.to_csv(f, index=False)\n",
    "#     else:\n",
    "#         with open(csv_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#             subdf.to_csv(f, index=False)\n",
    "#     print(f\"Saved {len(subdf)} train rows => {csv_path}\")\n",
    "\n",
    "#     # JSON: standard DPO format\n",
    "#     out_list = []\n",
    "#     for row in subdf.to_dict(orient=\"records\"):\n",
    "#         # build conversation\n",
    "#         conv = []\n",
    "#         for turn in row[\"conversations\"]:\n",
    "#             speaker = \"human\" if turn[\"role\"] == \"user\" else \"gpt\"\n",
    "#             conv.append({\n",
    "#                 \"from\": speaker,\n",
    "#                 \"value\": turn[\"content\"]\n",
    "#             })\n",
    "#         item = {\n",
    "#             \"conversations\": conv,\n",
    "#             \"chosen\": {\n",
    "#                 \"from\": \"gpt\",\n",
    "#                 \"value\": row[\"chosen\"]\n",
    "#             },\n",
    "#             \"rejected\": {\n",
    "#                 \"from\": \"gpt\",\n",
    "#                 \"value\": row[\"rejected\"]\n",
    "#             },\n",
    "#             \"meta\": {\n",
    "#                 \"qn_id\": row[\"qn_id\"],\n",
    "#                 \"split\": row[\"split\"],\n",
    "#                 \"sample_subset\": row[\"sample_subset\"],\n",
    "#                 \"appeal_technique\": row[\"appeal_technique\"],\n",
    "#                 \"appeal_setting\": row[\"appeal_setting\"],\n",
    "#                 \"num_turns\": row[\"num_turns\"],\n",
    "#                 \"source\": row[\"source\"],\n",
    "#                 \"category\": row[\"category\"],\n",
    "#                 \"correct_letter\": row[\"correct_letter\"],\n",
    "#                 \"target_letter\": row[\"target_letter\"],\n",
    "#                 \"initial_model_letter\": row[\"initial_model_letter\"]\n",
    "                \n",
    "#             }\n",
    "#         }\n",
    "#         out_list.append(item)\n",
    "\n",
    "#     json_ext = \".json.gz\" if use_gzip else \".json\"\n",
    "#     json_path = os.path.join(OUT_DIR, f\"{name}{json_ext}\")\n",
    "#     if use_gzip:\n",
    "#         with gzip.open(json_path, \"wt\", encoding=\"utf-8\") as f:\n",
    "#             json.dump(out_list, f, ensure_ascii=False, indent=2)\n",
    "#     else:\n",
    "#         with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#             json.dump(out_list, f, ensure_ascii=False, indent=2)\n",
    "#     print(f\"Saved {len(out_list)} train DPO pairs => {json_path}\")\n",
    "\n",
    "#     # KTO version => double the sample (one with kto_tag=True => chosen, one with kto_tag=False => rejected)\n",
    "#     kto_out = []\n",
    "#     for row in subdf.to_dict(orient=\"records\"):\n",
    "#         # build conversation\n",
    "#         base_conv = []\n",
    "#         for turn in row[\"conversations\"]:\n",
    "#             speaker = \"human\" if turn[\"role\"] == \"user\" else \"gpt\"\n",
    "#             base_conv.append({\"from\": speaker, \"value\": turn[\"content\"]})\n",
    "\n",
    "#         chosen_item = {\n",
    "#             \"conversations\": base_conv,\n",
    "#             \"kto_tag\": True,\n",
    "#             \"meta\": {\n",
    "#                 \"qn_id\": row[\"qn_id\"],\n",
    "#                 \"split\": row[\"split\"],\n",
    "#                 \"sample_subset\": row[\"sample_subset\"]\n",
    "#             }\n",
    "#         }\n",
    "#         kto_out.append(chosen_item)\n",
    "\n",
    "#         rejected_item = {\n",
    "#             \"conversations\": base_conv,\n",
    "#             \"kto_tag\": False,\n",
    "#             \"meta\": {\n",
    "#                 \"qn_id\": row[\"qn_id\"],\n",
    "#                 \"split\": row[\"split\"],\n",
    "#                 \"sample_subset\": row[\"sample_subset\"],\n",
    "#                 \"appeal_technique\": row[\"appeal_technique\"],\n",
    "#                 \"appeal_setting\": row[\"appeal_setting\"],\n",
    "#                 \"num_turns\": row[\"num_turns\"],\n",
    "#                 \"source\": row[\"source\"],\n",
    "#                 \"category\": row[\"category\"],\n",
    "#                 \"correct_letter\": row[\"correct_letter\"],\n",
    "#                 \"target_letter\": row[\"target_letter\"],\n",
    "#                 \"initial_model_letter\": row[\"initial_model_letter\"]\n",
    "#             }\n",
    "#         }\n",
    "#         kto_out.append(rejected_item)\n",
    "\n",
    "#     kto_ext = \"_kto.json.gz\" if use_gzip else \"_kto.json\"\n",
    "#     kto_json_path = os.path.join(OUT_DIR, f\"{name}{kto_ext}\")\n",
    "#     if use_gzip:\n",
    "#         with gzip.open(kto_json_path, \"wt\", encoding=\"utf-8\") as f:\n",
    "#             json.dump(kto_out, f, ensure_ascii=False, indent=2)\n",
    "#     else:\n",
    "#         with open(kto_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#             json.dump(kto_out, f, ensure_ascii=False, indent=2)\n",
    "#     print(f\"Saved {len(kto_out)} train KTO records => {kto_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 345 train rows => dpo_datasets/baseline.csv\n",
      "Saved 345 train DPO pairs => dpo_datasets/baseline.json\n",
      "Saved 690 train KTO records => dpo_datasets/baseline_kto.json\n",
      "Saved 6939 train rows => dpo_datasets/resist.csv\n",
      "Saved 6939 train DPO pairs => dpo_datasets/resist.json\n",
      "Saved 13878 train KTO records => dpo_datasets/resist_kto.json\n",
      "Saved 14835 train rows => dpo_datasets/holistic.csv\n",
      "Saved 14835 train DPO pairs => dpo_datasets/holistic.json\n",
      "Saved 29670 train KTO records => dpo_datasets/holistic_kto.json\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # 1) baseline only\n",
    "# cond_baseline = (df[\"sample_subset\"] == \"baseline\")\n",
    "# filter_and_save(\"baseline\", cond_baseline, use_gzip=False)\n",
    "# # filter_and_save(\"baseline\", cond_baseline, use_gzip=True)\n",
    "\n",
    "\n",
    "# # 2) baseline + resist_NEG\n",
    "# cond_resist = cond_baseline | (df[\"sample_subset\"] == \"resist_NEG\")\n",
    "# filter_and_save(\"resist\", cond_resist, use_gzip=False)\n",
    "# # filter_and_save(\"resistNEGplusBaseline\", cond_resist, use_gzip=True)\n",
    "\n",
    "# # 3) baseline + resist_NEG + relent_POS => \"holistic\"\n",
    "# cond_holistic = cond_baseline | (df[\"sample_subset\"] == \"resist_NEG\") | (df[\"sample_subset\"] == \"relent_POS\")\n",
    "# filter_and_save(\"holistic\", cond_holistic, use_gzip=False)\n",
    "# # filter_and_save(\"holistic\", cond_holistic, use_gzip=True)\n",
    "\n",
    "# # # 4) if you want only \"relent_POS\" alone\n",
    "# # cond_relent = (df[\"sample_subset\"] == \"relent_POS\")\n",
    "# # filter_and_save(\"relentPOS\", cond_relent)\n",
    "\n",
    "# print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Subset distribution ###\n",
      "sample_subset  split\n",
      "baseline       test      1122\n",
      "               train     1124\n",
      "relent_POS     test     22638\n",
      "               train    22722\n",
      "resist_NEG     test     24486\n",
      "               train    24486\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 4) Use data/qn_id_split_50_50_increments.json to see if train or test\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "QNSPLIT_INCR_JSON = \"data/qn_id_split_50_50_increments_llama31_8b.json\"  # new file\n",
    "\n",
    "with open(QNSPLIT_INCR_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    split_data = json.load(f)\n",
    "\n",
    "# Each key is e.g. train_ids_10, train_ids_20, ..., train_ids_50, plus test_ids\n",
    "# We'll store them in sets:\n",
    "train_increments = {\n",
    "    10: set(split_data[\"train_ids_10\"]),\n",
    "    20: set(split_data[\"train_ids_20\"]),\n",
    "    30: set(split_data[\"train_ids_30\"]),\n",
    "    40: set(split_data[\"train_ids_40\"]),\n",
    "    50: set(split_data[\"train_ids_50\"]),\n",
    "}\n",
    "test_ids = set(split_data[\"test_ids\"])\n",
    "\n",
    "def get_split(qn_id):\n",
    "    # We'll label 'test' if qn_id in test_ids, else 'train' if in the final (50%) train_ids_50,\n",
    "    # else 'unspecified' if it wasn't found (shouldn't happen if the entire dataset is 50/50).\n",
    "    # This is optional. You might skip if you only plan to filter by the inc sets.\n",
    "    if qn_id in test_ids:\n",
    "        return \"test\"\n",
    "    elif qn_id in train_increments[50]:\n",
    "        return \"train\"\n",
    "    else:\n",
    "        return \"unspecified\"\n",
    "\n",
    "df[\"split\"] = df[\"qn_id\"].apply(get_split)\n",
    "\n",
    "# Print overall distribution\n",
    "grouped = df.groupby([\"sample_subset\", \"split\"]).size()\n",
    "print(\"\\n### Subset distribution ###\")\n",
    "print(grouped)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5) We'll define the same filter_and_save function,\n",
    "#    but also show how to loop over increments [10, 20, 30, 40, 50].\n",
    "# ------------------------------------------------------------------------------\n",
    "import os\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "def filter_and_save(name, condition, qn_id_set, inc_label=None, use_gzip=False):\n",
    "    \"\"\"\n",
    "    1) subdf = df[condition & (df['qn_id'] in qn_id_set)] => i.e. the portion of that subset\n",
    "       that belongs in this train increment\n",
    "    2) Save in CSV, JSON, KTO formats, with some naming pattern\n",
    "    \"\"\"\n",
    "    subdf = df[condition & df[\"qn_id\"].isin(qn_id_set)].copy()\n",
    "    if subdf.empty:\n",
    "        print(f\"Skipping {name}, no rows found for inc={inc_label}\")\n",
    "        return\n",
    "\n",
    "    suffix = f\"_{inc_label}\" if inc_label else \"\"  # e.g. \"_10\", \"_20\"\n",
    "    # CSV\n",
    "    csv_ext = \".csv.gz\" if use_gzip else \".csv\"\n",
    "    csv_path = os.path.join(OUT_DIR, f\"{name}{suffix}{csv_ext}\")\n",
    "    if use_gzip:\n",
    "        with gzip.open(csv_path, \"wt\", encoding=\"utf-8\") as f:\n",
    "            subdf.to_csv(f, index=False)\n",
    "    else:\n",
    "        subdf.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved {len(subdf)} rows => {csv_path}\")\n",
    "\n",
    "    # JSON in DPO format\n",
    "    out_list = []\n",
    "    for row in subdf.to_dict(orient=\"records\"):\n",
    "        conv = []\n",
    "        for turn in row[\"conversations\"]:\n",
    "            speaker = \"human\" if turn[\"role\"]==\"user\" else \"gpt\"\n",
    "            conv.append({\"from\": speaker, \"value\": turn[\"content\"]})\n",
    "\n",
    "        item = {\n",
    "            \"conversations\": conv,\n",
    "            \"chosen\": {\"from\": \"gpt\", \"value\": row[\"chosen\"]},\n",
    "            \"rejected\": {\"from\": \"gpt\", \"value\": row[\"rejected\"]},\n",
    "            \"meta\": {\n",
    "                \"qn_id\": row[\"qn_id\"],\n",
    "                \"split\": row[\"split\"],  # e.g. 'test' or 'train'\n",
    "                \"sample_subset\": row[\"sample_subset\"],\n",
    "                \"appeal_technique\": row[\"appeal_technique\"],\n",
    "                \"appeal_setting\": row[\"appeal_setting\"],\n",
    "                \"num_turns\": row[\"num_turns\"],\n",
    "                \"source\": row[\"source\"],\n",
    "                \"category\": row[\"category\"],\n",
    "                \"correct_letter\": row[\"correct_letter\"],\n",
    "                \"target_letter\": row[\"target_letter\"],\n",
    "                \"initial_model_letter\": row[\"initial_model_letter\"]\n",
    "            }\n",
    "        }\n",
    "        out_list.append(item)\n",
    "\n",
    "    json_ext = \".json.gz\" if use_gzip else \".json\"\n",
    "    json_path = os.path.join(OUT_DIR, f\"{name}{suffix}{json_ext}\")\n",
    "    if use_gzip:\n",
    "        with gzip.open(json_path, \"wt\", encoding=\"utf-8\") as f:\n",
    "            json.dump(out_list, f, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(out_list, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved {len(out_list)} DPO pairs => {json_path}\")\n",
    "\n",
    "    # KTO version\n",
    "    kto_out = []\n",
    "    for row in subdf.to_dict(orient=\"records\"):\n",
    "        base_conv = []\n",
    "        for turn in row[\"conversations\"]:\n",
    "            speaker = \"human\" if turn[\"role\"]==\"user\" else \"gpt\"\n",
    "            base_conv.append({\"from\": speaker, \"value\": turn[\"content\"]})\n",
    "\n",
    "        chosen_item = {\n",
    "            \"conversations\": base_conv,\n",
    "            \"kto_tag\": True,\n",
    "            \"meta\": {\n",
    "                \"qn_id\": row[\"qn_id\"],\n",
    "                \"split\": row[\"split\"],\n",
    "                \"sample_subset\": row[\"sample_subset\"],\n",
    "                \"appeal_technique\": row[\"appeal_technique\"],\n",
    "                \"appeal_setting\": row[\"appeal_setting\"],\n",
    "                \"num_turns\": row[\"num_turns\"],\n",
    "                \"source\": row[\"source\"],\n",
    "                \"category\": row[\"category\"],\n",
    "                \"correct_letter\": row[\"correct_letter\"],\n",
    "                \"target_letter\": row[\"target_letter\"],\n",
    "                \"initial_model_letter\": row[\"initial_model_letter\"]\n",
    "            }\n",
    "        }\n",
    "        kto_out.append(chosen_item)\n",
    "\n",
    "        rejected_item = {\n",
    "            \"conversations\": base_conv,\n",
    "            \"kto_tag\": False,\n",
    "            \"meta\": {\n",
    "                \"qn_id\": row[\"qn_id\"],\n",
    "                \"split\": row[\"split\"],\n",
    "                \"sample_subset\": row[\"sample_subset\"],\n",
    "                \"appeal_technique\": row[\"appeal_technique\"],\n",
    "                \"appeal_setting\": row[\"appeal_setting\"],\n",
    "                \"num_turns\": row[\"num_turns\"],\n",
    "                \"source\": row[\"source\"],\n",
    "                \"category\": row[\"category\"],\n",
    "                \"correct_letter\": row[\"correct_letter\"],\n",
    "                \"target_letter\": row[\"target_letter\"],\n",
    "                \"initial_model_letter\": row[\"initial_model_letter\"]\n",
    "            }\n",
    "        }\n",
    "        kto_out.append(rejected_item)\n",
    "\n",
    "    kto_ext = \"_kto.json.gz\" if use_gzip else \"_kto.json\"\n",
    "    kto_json_path = os.path.join(OUT_DIR, f\"{name}{suffix}{kto_ext}\")\n",
    "    if use_gzip:\n",
    "        with gzip.open(kto_json_path, \"wt\", encoding=\"utf-8\") as f:\n",
    "            json.dump(kto_out, f, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        with open(kto_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(kto_out, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved {len(kto_out)} KTO records => {kto_json_path}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 238 rows => dpo_datasets/baseline_10.csv.gz\n",
      "Saved 238 DPO pairs => dpo_datasets/baseline_10.json.gz\n",
      "Saved 476 KTO records => dpo_datasets/baseline_10_kto.json.gz\n",
      "Saved 5404 rows => dpo_datasets/resist_10.csv.gz\n",
      "Saved 5404 DPO pairs => dpo_datasets/resist_10.json.gz\n",
      "Saved 10808 KTO records => dpo_datasets/resist_10_kto.json.gz\n",
      "Saved 10234 rows => dpo_datasets/holistic_10.csv.gz\n",
      "Saved 10234 DPO pairs => dpo_datasets/holistic_10.json.gz\n",
      "Saved 20468 KTO records => dpo_datasets/holistic_10_kto.json.gz\n",
      "Saved 474 rows => dpo_datasets/baseline_20.csv.gz\n",
      "Saved 474 DPO pairs => dpo_datasets/baseline_20.json.gz\n",
      "Saved 948 KTO records => dpo_datasets/baseline_20_kto.json.gz\n",
      "Saved 10764 rows => dpo_datasets/resist_20.csv.gz\n",
      "Saved 10764 DPO pairs => dpo_datasets/resist_20.json.gz\n",
      "Saved 21528 KTO records => dpo_datasets/resist_20_kto.json.gz\n",
      "Saved 20382 rows => dpo_datasets/holistic_20.csv.gz\n",
      "Saved 20382 DPO pairs => dpo_datasets/holistic_20.json.gz\n",
      "Saved 40764 KTO records => dpo_datasets/holistic_20_kto.json.gz\n",
      "Saved 710 rows => dpo_datasets/baseline_30.csv.gz\n",
      "Saved 710 DPO pairs => dpo_datasets/baseline_30.json.gz\n",
      "Saved 1420 KTO records => dpo_datasets/baseline_30_kto.json.gz\n",
      "Saved 16124 rows => dpo_datasets/resist_30.csv.gz\n",
      "Saved 16124 DPO pairs => dpo_datasets/resist_30.json.gz\n",
      "Saved 32248 KTO records => dpo_datasets/resist_30_kto.json.gz\n",
      "Saved 30530 rows => dpo_datasets/holistic_30.csv.gz\n",
      "Saved 30530 DPO pairs => dpo_datasets/holistic_30.json.gz\n",
      "Saved 61060 KTO records => dpo_datasets/holistic_30_kto.json.gz\n",
      "Saved 945 rows => dpo_datasets/baseline_40.csv.gz\n",
      "Saved 945 DPO pairs => dpo_datasets/baseline_40.json.gz\n",
      "Saved 1890 KTO records => dpo_datasets/baseline_40_kto.json.gz\n",
      "Saved 21441 rows => dpo_datasets/resist_40.csv.gz\n",
      "Saved 21441 DPO pairs => dpo_datasets/resist_40.json.gz\n",
      "Saved 42882 KTO records => dpo_datasets/resist_40_kto.json.gz\n",
      "Saved 40635 rows => dpo_datasets/holistic_40.csv.gz\n",
      "Saved 40635 DPO pairs => dpo_datasets/holistic_40.json.gz\n",
      "Saved 81270 KTO records => dpo_datasets/holistic_40_kto.json.gz\n",
      "Saved 1124 rows => dpo_datasets/baseline_50.csv.gz\n",
      "Saved 1124 DPO pairs => dpo_datasets/baseline_50.json.gz\n",
      "Saved 2248 KTO records => dpo_datasets/baseline_50_kto.json.gz\n",
      "Saved 25610 rows => dpo_datasets/resist_50.csv.gz\n",
      "Saved 25610 DPO pairs => dpo_datasets/resist_50.json.gz\n",
      "Saved 51220 KTO records => dpo_datasets/resist_50_kto.json.gz\n",
      "Saved 48332 rows => dpo_datasets/holistic_50.csv.gz\n",
      "Saved 48332 DPO pairs => dpo_datasets/holistic_50.json.gz\n",
      "Saved 96664 KTO records => dpo_datasets/holistic_50_kto.json.gz\n",
      "All done with incremental subsets!\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Now produce the subsets for inc in [10,20,30,40,50].\n",
    "# We'll define the same baseline/resist/holistic logic for each increment\n",
    "# ------------------------------------------------------------------------------\n",
    "cond_baseline  = (df[\"sample_subset\"]==\"baseline\")\n",
    "cond_resist    = cond_baseline | (df[\"sample_subset\"]==\"resist_NEG\")\n",
    "cond_holistic  = cond_baseline | (df[\"sample_subset\"]==\"resist_NEG\") | (df[\"sample_subset\"]==\"relent_POS\")\n",
    "\n",
    "for inc in [10,20,30,40,50]:\n",
    "    train_ids_inc = train_increments[inc]  # set of qn_ids\n",
    "\n",
    "    filter_and_save(\"baseline\",  cond_baseline,  train_ids_inc, inc_label=str(inc), use_gzip=True)\n",
    "    filter_and_save(\"resist\",    cond_resist,    train_ids_inc, inc_label=str(inc), use_gzip=True)\n",
    "    filter_and_save(\"holistic\",  cond_holistic,  train_ids_inc, inc_label=str(inc), use_gzip=True)\n",
    "\n",
    "print(\"All done with incremental subsets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamafactory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
